{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmNC41FfUR1m",
        "outputId": "1aa4ed5f-4836-422c-8ac2-7503a060c3ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Download 'punkt_tab' data package\n",
        "nltk.download('punkt_tab') # Download the required data package. This data is not installed by default\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "url = \"https://huggingface.co/datasets/sleeping-ai/TEKGEN-Wiki\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "# Save the data to a file\n",
        "with open(\"dataset.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(data)\n",
        "\n",
        "## Load the dataset from file\n",
        "with open('dataset.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "# Tokenization\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "\n",
        "# Lowercasing\n",
        "tokens = [token.lower() for token in tokens]\n",
        "\n",
        "\n",
        "# Removing punctuation and special characters\n",
        "tokens = [re.sub(r'\\W+', '', token) for token in tokens if re.sub(r'\\W+', '', token)]\n",
        "\n",
        "# Removing stop words (optional)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Add a special token for unknown words\n",
        "tokens.append('')"
      ],
      "metadata": {
        "id": "zsKItVLbUigX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numericalization\n",
        "vocab = list(set(tokens))\n",
        "word2index = {word: i for i, word in enumerate(vocab)}\n",
        "index2word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "# Creating sequences\n",
        "sequence_length = 5\n",
        "sequences = []\n",
        "for i in range(len(tokens) - sequence_length):\n",
        "    sequences.append(tokens[i:i + sequence_length])\n",
        "\n",
        "\n",
        "\n",
        "# Convert sequences to numerical indices\n",
        "input_sequences = []\n",
        "for sequence in sequences:\n",
        "    input_sequences.append([word2index[word] for word in sequence])\n",
        "\n",
        "\n",
        "\n",
        "# Convert to numpy array\n",
        "input_sequences = np.array(input_sequences)\n",
        "\n",
        "print(f\"Total sequences: {len(input_sequences)}\")\n",
        ""
      ],
      "metadata": {
        "id": "1ZvJKok9Uzf6",
        "outputId": "6f51336f-2203-412a-f6fb-90f9dc19e474",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences: 21210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        x = self.embedding(x)\n",
        "        x, state = self.lstm(x, prev_state)\n",
        "        x = self.fc(x)\n",
        "        return x, state\n",
        "\n",
        "    def init_state(self, batch_size=1):\n",
        "        return (torch.zeros(2, batch_size, self.lstm.hidden_size),\n",
        "                torch.zeros(2, batch_size, self.lstm.hidden_size))"
      ],
      "metadata": {
        "id": "FkW0uaqLU2sw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 50\n",
        "hidden_dim = 100\n",
        "vocab_size = len(vocab)\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "# Model, loss function, optimizer\n",
        "model = LanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, input_sequences, criterion, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(0, len(input_sequences) - batch_size, batch_size):\n",
        "            inputs = torch.tensor(input_sequences[i:i + batch_size, :-1], dtype=torch.long)\n",
        "            targets = torch.tensor(input_sequences[i:i + batch_size, 1:], dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            state_h, state_c = model.init_state(batch_size)\n",
        "            outputs, _ = model(inputs, (state_h, state_c))\n",
        "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / (len(input_sequences) // batch_size)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, input_sequences, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "id": "Ml4xcZayU9bO",
        "outputId": "dbc32e51-8f57-4351-87e1-d18b5b441103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 6.6691\n",
            "Epoch 2/20, Loss: 5.1554\n",
            "Epoch 3/20, Loss: 4.5338\n",
            "Epoch 4/20, Loss: 3.9544\n",
            "Epoch 5/20, Loss: 3.4100\n",
            "Epoch 6/20, Loss: 2.9453\n",
            "Epoch 7/20, Loss: 2.5522\n",
            "Epoch 8/20, Loss: 2.2179\n",
            "Epoch 9/20, Loss: 1.9371\n",
            "Epoch 10/20, Loss: 1.6999\n",
            "Epoch 11/20, Loss: 1.5012\n",
            "Epoch 12/20, Loss: 1.3370\n",
            "Epoch 13/20, Loss: 1.2047\n",
            "Epoch 14/20, Loss: 1.0987\n",
            "Epoch 15/20, Loss: 1.0114\n",
            "Epoch 16/20, Loss: 0.9410\n",
            "Epoch 17/20, Loss: 0.8827\n",
            "Epoch 18/20, Loss: 0.8373\n",
            "Epoch 19/20, Loss: 0.8029\n",
            "Epoch 20/20, Loss: 0.7688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "id": "-lnmFX8QVAV8",
        "outputId": "91bf2f27-9db9-4fd9-dfb0-8ee2fd226aad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def generate_text(model, start_text, max_length=50):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "    state_h, state_c = model.init_state(batch_size=1)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Use the word2index dictionary with a fallback to '' if the word is not found\n",
        "        x = torch.tensor([[word2index.get(w, word2index['']) for w in words]], dtype=torch.long)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(index2word[word_index])\n",
        "\n",
        "    # Remove numerical characters from the generated words\n",
        "    filtered_words = [re.sub(r'\\d+', '', word) for word in words]\n",
        "\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Test the model with a sample input\n",
        "start_text = \"harry potter is\"\n",
        "generated_text = generate_text(model, start_text)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "nmxAouvvVDqu",
        "outputId": "e83579a3-7823-4df3-efdb-54aa03fc2616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "harry potter is path mvhvhmhavahavaz px button svg class mr textgreen span class rel hover underline href pricing pricing li li class hover textyellow class group flex itemscenter class minwfit maxwsm breakwords p div class maxwfull svg class btn flex itemscenter overflowhidden whitespacenowrap fontsemibold science bag started colleague glenn schmieg  div div\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "pa2RLLpgVPyD",
        "outputId": "0e573d18-d0a0-4c65-fe69-5a7e74c41297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.23.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.41.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Define the model class (this should match your trained model's class)\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        x = self.embedding(x)\n",
        "        x, state = self.lstm(x, prev_state)\n",
        "        x = self.fc(x)\n",
        "        return x, state\n",
        "\n",
        "    def init_state(self, batch_size=1):\n",
        "        return (torch.zeros(2, batch_size, self.lstm.hidden_size),\n",
        "                torch.zeros(2, batch_size, self.lstm.hidden_size))\n",
        "# Load the trained model\n",
        "model = LanguageModel(vocab_size=len(vocab), embedding_dim=50, hidden_dim=100)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Define the text generation function\n",
        "def generate_text(model, start_text, max_length=50):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "    state_h, state_c = model.init_state(batch_size=1)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Use the word2index dictionary with a fallback to  if the word is not found\n",
        "        x = torch.tensor([[word2index.get(w, word2index['']) for w in words]], dtype=torch.long)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(index2word[word_index])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Streamlit app interface\n",
        "st.title(\"Text Generation with Language Model\")\n",
        "st.write(\"Enter a text prompt and the model will generate a continuation of the text.\")\n",
        "\n",
        "# Input box for user to type in a text prompt\n",
        "user_input = st.text_input(\"Enter text prompt:\", \"Harry Potter is\")\n",
        "\n",
        "# Generate text based on user input\n",
        "if user_input:\n",
        "    with st.spinner('Generating text...'):\n",
        "        generated_text = generate_text(model, user_input)\n",
        "        st.success(\"Generated Text:\")\n",
        "        st.write(generated_text)\n",
        ""
      ],
      "metadata": {
        "id": "McVfmOtfWsZK",
        "outputId": "7e670cd5-d096-4128-ab95-83fc0338bac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-e9e71f68d2be>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('model.pth'))\n",
            "2025-01-26 20:43:50.997 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.198 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-01-26 20:43:51.201 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.206 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.210 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.213 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.217 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.218 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.220 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.221 Session state does not function when running a script without `streamlit run`\n",
            "2025-01-26 20:43:51.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.226 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.227 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.228 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.230 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.415 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.417 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.420 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.422 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.424 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.425 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 20:43:51.428 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mk5eqrt4Wzhc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}